{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch101.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMy+hjA3XMWF4kS4kQB69yI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bassoline/DeepLearning/blob/main/PyTorch101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqWlHlysNLpP",
        "outputId": "90cc1d17-12ec-474d-d1f3-6b5be4d34ca3"
      },
      "source": [
        "import torch \r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim \r\n",
        "import torch.nn.functional as F \r\n",
        "\r\n",
        "# for reducibility \r\n",
        "torch.manual_seed(0)\r\n",
        "\r\n",
        "# define the model  (functional)\r\n",
        "class MLP_f(nn.Module): \r\n",
        "  def __init__(self, num_inputs, num_hidden_layer_nodes, num_outputs): \r\n",
        "    # Initialize super class \r\n",
        "    super().__init__()\r\n",
        "\r\n",
        "    # Add hidden layer\r\n",
        "    self.hidden1 = nn.Linear(num_inputs, num_hidden_layer_nodes)\r\n",
        "\r\n",
        "    # Add output layer\r\n",
        "    self.output = nn.Linear(num_hidden_layer_nodes, num_outputs)\r\n",
        "  \r\n",
        "  def forward(self, x): \r\n",
        "    # forward pass through hidden layers with relu activation function \r\n",
        "    x = F.relu(self.hidden1(x))\r\n",
        "    # foward pass to output layer \r\n",
        "    return self.output(x)\r\n",
        "\r\n",
        "\r\n",
        "class MLP_s(nn.Module): \r\n",
        "  def __init__(self, num_inputs, num_hidden_layer_nodes, num_outputs):\r\n",
        "    # Initialize super class \r\n",
        "    super().__init__()\r\n",
        "\r\n",
        "    # build model sequentially \r\n",
        "    self.model = nn.Sequential(\r\n",
        "        # add hidden layer \r\n",
        "        nn.Linear(num_inputs, num_hidden_layer_nodes),\r\n",
        "        # add ReLU activation \r\n",
        "        nn.ReLU(), \r\n",
        "        # add output layer \r\n",
        "        nn.Linear(num_hidden_layer_nodes, num_outputs)\r\n",
        "    )\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    # Forward pass \r\n",
        "    return self.model(x)    \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# num data points \r\n",
        "num_data = 1000 \r\n",
        "\r\n",
        "# network parameters \r\n",
        "num_inputs = 1000\r\n",
        "num_hidden_layer_nodes = 100 \r\n",
        "num_outputs = 10 \r\n",
        "\r\n",
        "# training parameters\r\n",
        "num_epochs = 100 \r\n",
        "\r\n",
        "# create random Tensors to hold inputs and outputs \r\n",
        "x = torch.randn(num_data, num_inputs)\r\n",
        "y = torch.randn(num_data, num_outputs)\r\n",
        "\r\n",
        "# create models\r\n",
        "# model_type = 'functional'\r\n",
        "# model = MLP_f(num_inputs, num_hidden_layer_nodes, num_outputs)\r\n",
        "model_type = 'sequential'\r\n",
        "model = MLP_s(num_inputs, num_hidden_layer_nodes, num_outputs)\r\n",
        "\r\n",
        "# Define loss function\r\n",
        "loss_function = nn.MSELoss(reduction='sum')\r\n",
        "\r\n",
        "# Define optimizer\r\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-4)\r\n",
        "\r\n",
        "\r\n",
        "for t in range(num_epochs):\r\n",
        "\r\n",
        "    # Forward pass: Compute predicted y by passing x to the model\r\n",
        "    y_pred = model(x)\r\n",
        "\r\n",
        "    # Compute and print loss\r\n",
        "    loss = loss_function(y_pred, y)\r\n",
        "    print(model_type, t, loss.item())\r\n",
        "\r\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\r\n",
        "    optimizer.zero_grad()\r\n",
        "\r\n",
        "    # Calculate gradient using backward pass\r\n",
        "    loss.backward()\r\n",
        "\r\n",
        "    # Update model parameters (weights)\r\n",
        "    optimizer.step()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sequential 0 10581.46484375\n",
            "sequential 1 9755.71875\n",
            "sequential 2 9161.2998046875\n",
            "sequential 3 8637.4951171875\n",
            "sequential 4 8135.08154296875\n",
            "sequential 5 7634.916015625\n",
            "sequential 6 7127.3212890625\n",
            "sequential 7 6610.97021484375\n",
            "sequential 8 6087.37255859375\n",
            "sequential 9 5563.396484375\n",
            "sequential 10 5048.94873046875\n",
            "sequential 11 4551.427734375\n",
            "sequential 12 4078.5126953125\n",
            "sequential 13 3634.744873046875\n",
            "sequential 14 3225.676025390625\n",
            "sequential 15 2852.95361328125\n",
            "sequential 16 2516.31201171875\n",
            "sequential 17 2214.4755859375\n",
            "sequential 18 1946.5374755859375\n",
            "sequential 19 1709.10693359375\n",
            "sequential 20 1499.0341796875\n",
            "sequential 21 1313.603515625\n",
            "sequential 22 1151.3836669921875\n",
            "sequential 23 1009.0325317382812\n",
            "sequential 24 883.9706420898438\n",
            "sequential 25 774.8304443359375\n",
            "sequential 26 679.212646484375\n",
            "sequential 27 596.10986328125\n",
            "sequential 28 524.0169677734375\n",
            "sequential 29 462.084228515625\n",
            "sequential 30 410.6278381347656\n",
            "sequential 31 371.1553039550781\n",
            "sequential 32 350.0204772949219\n",
            "sequential 33 362.756591796875\n",
            "sequential 34 447.5335693359375\n",
            "sequential 35 690.4537353515625\n",
            "sequential 36 1283.8463134765625\n",
            "sequential 37 2541.542724609375\n",
            "sequential 38 4725.90625\n",
            "sequential 39 6823.033203125\n",
            "sequential 40 6313.8349609375\n",
            "sequential 41 2986.13623046875\n",
            "sequential 42 998.749267578125\n",
            "sequential 43 445.49993896484375\n",
            "sequential 44 290.16455078125\n",
            "sequential 45 217.7805633544922\n",
            "sequential 46 172.62179565429688\n",
            "sequential 47 140.7528076171875\n",
            "sequential 48 116.93134307861328\n",
            "sequential 49 98.4639663696289\n",
            "sequential 50 83.79827880859375\n",
            "sequential 51 71.95231628417969\n",
            "sequential 52 62.27023696899414\n",
            "sequential 53 54.22121047973633\n",
            "sequential 54 47.450504302978516\n",
            "sequential 55 41.73115539550781\n",
            "sequential 56 36.833431243896484\n",
            "sequential 57 32.64141082763672\n",
            "sequential 58 29.01911163330078\n",
            "sequential 59 25.8761043548584\n",
            "sequential 60 23.137428283691406\n",
            "sequential 61 20.735118865966797\n",
            "sequential 62 18.614917755126953\n",
            "sequential 63 16.752655029296875\n",
            "sequential 64 15.102555274963379\n",
            "sequential 65 13.6406831741333\n",
            "sequential 66 12.338808059692383\n",
            "sequential 67 11.178815841674805\n",
            "sequential 68 10.143546104431152\n",
            "sequential 69 9.220939636230469\n",
            "sequential 70 8.391463279724121\n",
            "sequential 71 7.6475114822387695\n",
            "sequential 72 6.976708889007568\n",
            "sequential 73 6.376175880432129\n",
            "sequential 74 5.834391117095947\n",
            "sequential 75 5.345690727233887\n",
            "sequential 76 4.902877330780029\n",
            "sequential 77 4.502501010894775\n",
            "sequential 78 4.139337062835693\n",
            "sequential 79 3.8073043823242188\n",
            "sequential 80 3.50665020942688\n",
            "sequential 81 3.232914686203003\n",
            "sequential 82 2.9829838275909424\n",
            "sequential 83 2.755523920059204\n",
            "sequential 84 2.5481531620025635\n",
            "sequential 85 2.3577229976654053\n",
            "sequential 86 2.1838197708129883\n",
            "sequential 87 2.0240378379821777\n",
            "sequential 88 1.8779643774032593\n",
            "sequential 89 1.7442824840545654\n",
            "sequential 90 1.6204639673233032\n",
            "sequential 91 1.5072649717330933\n",
            "sequential 92 1.4024895429611206\n",
            "sequential 93 1.3062214851379395\n",
            "sequential 94 1.2176371812820435\n",
            "sequential 95 1.1358096599578857\n",
            "sequential 96 1.0603009462356567\n",
            "sequential 97 0.9905630350112915\n",
            "sequential 98 0.9260714650154114\n",
            "sequential 99 0.8663493394851685\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1qZsSWGTEVe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}