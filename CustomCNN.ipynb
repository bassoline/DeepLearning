{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CustomCNN.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOCF9hE6kJOl1tbS+HiLtx8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bassoline/DeepLearning/blob/main/CustomCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvLAm2zCu-Mw"
      },
      "source": [
        "%matplotlib inline \n",
        "import matplotlib.pyplot as plt \n",
        "import os \n",
        "import time \n",
        "from dataclasses import dataclass \n",
        "import numpy as np \n",
        "import torch \n",
        "import torch.nn as nn \n",
        "import torch.optim as optim \n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim import lr_scheduler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0SZ254S2xFH"
      },
      "source": [
        "required_training = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ancBJHmRvI-s"
      },
      "source": [
        "class MyModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # convolution layers\n",
        "    self._body = nn.Sequential(\n",
        "        # input size = (32, 32), output size = (28, 28)\n",
        "        nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5),\n",
        "        nn.BatchNorm2d(6),\n",
        "        nn.ReLU(inplace=True),\n",
        "        # forth conv layer input (28, 28) output is (24, 24)\n",
        "        nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5),\n",
        "        nn.BatchNorm2d(12),\n",
        "        nn.ReLU(inplace=True),\n",
        "        # third conv layer input (24, 24) output is (20, 20)\n",
        "        nn.Conv2d(in_channels=12, out_channels=24, kernel_size=5),\n",
        "        nn.BatchNorm2d(24),\n",
        "        nn.ReLU(inplace=True),\n",
        "        # forth conv layer input (20, 20) output is (8, 8)\n",
        "        nn.Conv2d(in_channels=24, out_channels=48, kernel_size=5),\n",
        "        nn.BatchNorm2d(48),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(2),\n",
        "        # forth conv layer input (8, 8) output is (3, 3)\n",
        "        nn.Conv2d(in_channels=48, out_channels=72, kernel_size=3),\n",
        "        nn.BatchNorm2d(72),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(2)\n",
        "    )\n",
        "\n",
        "    # fully connected layers \n",
        "    self._head = nn.Sequential(\n",
        "        # in features = total # of weights in last conv layer\n",
        "        nn.Linear(in_features=72*3*3, out_features=200), \n",
        "        nn.ReLU(inplace=True),\n",
        "        # last fully connected layer\n",
        "        nn.Linear(in_features=200, out_features=10)\n",
        "    )\n",
        "\n",
        "  def forward(self, x): \n",
        "    # apply feature extractor (conv layers)\n",
        "    x = self._body(x)\n",
        "    # flatten output of conv layer (dim should be batch_size * # of weights in \n",
        "    # last conv layer)\n",
        "    x = x.view(x.size()[0], -1) \n",
        "    # apply classifier \n",
        "    x = self._head(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcIi0dMevQ8B"
      },
      "source": [
        "# initalize and display network \n",
        "test_model = MyModel()\n",
        "print(test_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cncRDDutwP_F"
      },
      "source": [
        "@dataclass\n",
        "class SystemConfiguration:\n",
        "    '''\n",
        "    Describes the common system setting needed for reproducible training\n",
        "    '''\n",
        "    seed: int = 42  # seed number to set the state of all random number generators\n",
        "    cudnn_benchmark_enabled: bool = True  # enable CuDNN benchmark for the sake of performance\n",
        "    cudnn_deterministic: bool = True  # make cudnn deterministic (reproducible training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFkl2ZWEwljh"
      },
      "source": [
        "@dataclass\n",
        "class TrainingConfiguration:\n",
        "    '''\n",
        "    Describes configuration of the training process\n",
        "    '''\n",
        "    batch_size: int = 16  # amount of data to pass through the network at each forward-backward iteration\n",
        "    epochs_count: int = 20  # number of times the whole dataset will be passed through the network\n",
        "    learning_rate: float = 0.0005  # determines the speed of network's weights update\n",
        "        \n",
        "    log_interval: int = 100  # how many batches to wait between logging training status\n",
        "    test_interval: int = 1  # how many epochs to wait before another test. Set to 1 to get val loss at each epoch\n",
        "    data_root: str = \"./\"  # folder to save data\n",
        "    num_workers: int = 10  # number of concurrent processes using to prepare data\n",
        "    device: str = 'cuda'  # device to use for training.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy3rw4bgwHTR"
      },
      "source": [
        "def get_data(batch_size, data_root, num_workers=1):\n",
        "    \n",
        "    \n",
        "    try:\n",
        "        mean, std = get_mean_std_train_data(data_root)\n",
        "        assert len(mean) == len(std) == 3\n",
        "    except:\n",
        "        mean = np.array([0.5, 0.5, 0.5])\n",
        "        std = np.array([0.5, 0.5, 0.5])\n",
        "        \n",
        "    \n",
        "    common_transforms = transforms.Compose([\n",
        "        # this re-scale image tensor values between 0-1. image_tensor /= 255\n",
        "        transforms.ToTensor(),\n",
        "        # subtract mean and divide by variance.\n",
        "        transforms.Normalize(mean, std)\n",
        "    ])\n",
        "\n",
        "    data_augmentation = transforms.Compose([\n",
        "            transforms.RandomChoice([\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomVerticalFlip(),\n",
        "                transforms.RandomRotation(20),\n",
        "                transforms.RandomCrop(32, padding=4)\n",
        "            ]),\n",
        "            # this re-scale image tensor values between 0-1. image_tensor /= 255\n",
        "            transforms.ToTensor(),\n",
        "            # subtract mean and divide by variance.\n",
        "            transforms.Normalize(mean, std)\n",
        "    ])\n",
        "    \n",
        "    # train dataloader\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.CIFAR10(root=data_root, train=True, download=False, transform=data_augmentation),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "    \n",
        "    # test dataloader\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.CIFAR10(root=data_root, train=False, download=False, transform=common_transforms),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "    return train_loader, test_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ot6BqXHvWOV"
      },
      "source": [
        "def get_mean_std_train_data(data_root):\n",
        "    \n",
        "    train_transform = transforms.Compose([transforms.ToTensor()])\n",
        "    train_set = datasets.CIFAR10(root=data_root, train=True, download=True, transform=train_transform)\n",
        "    \n",
        "    # return mean (numpy.ndarray) and std (numpy.ndarray)\n",
        "    mean = np.mean(train_set.data.astype(np.float32), axis=(0, 1, 2))/255\n",
        "    std = np.std(train_set.data.astype(np.float32), axis=(0, 1, 2))\n",
        "    return mean, std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmrO_EwzwATO"
      },
      "source": [
        "def setup_system(system_config: SystemConfiguration) -> None:\n",
        "    torch.manual_seed(system_config.seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n",
        "        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5wxw8lcw5h6"
      },
      "source": [
        "def train(\n",
        "    train_config: TrainingConfiguration, model: nn.Module, optimizer: torch.optim.Optimizer,\n",
        "    train_loader: torch.utils.data.DataLoader, epoch_idx: int\n",
        ") -> None:\n",
        "    \n",
        "    # change model in training mood\n",
        "    model.train()\n",
        "    \n",
        "    # to get batch loss\n",
        "    batch_loss = np.array([])\n",
        "    \n",
        "    # to get batch accuracy\n",
        "    batch_acc = np.array([])\n",
        "        \n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        \n",
        "        # clone target\n",
        "        indx_target = target.clone()\n",
        "        # send data to device (its is medatory if GPU has to be used)\n",
        "        data = data.to(train_config.device)\n",
        "        # send target to device\n",
        "        target = target.to(train_config.device)\n",
        "\n",
        "        # reset parameters gradient to zero\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # forward pass to the model\n",
        "        output = model(data)\n",
        "        \n",
        "        # cross entropy loss\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        \n",
        "        # find gradients w.r.t training parameters\n",
        "        loss.backward()\n",
        "        # Update parameters using gardients\n",
        "        optimizer.step()\n",
        "        \n",
        "        batch_loss = np.append(batch_loss, [loss.item()])\n",
        "        \n",
        "        # Score to probability using softmax\n",
        "        prob = F.softmax(output, dim=1)\n",
        "            \n",
        "        # get the index of the max probability\n",
        "        pred = prob.data.max(dim=1)[1]  \n",
        "                        \n",
        "        # correct prediction\n",
        "        correct = pred.cpu().eq(indx_target).sum()\n",
        "            \n",
        "        # accuracy\n",
        "        acc = float(correct) / float(len(data))\n",
        "        \n",
        "        batch_acc = np.append(batch_acc, [acc])\n",
        "\n",
        "        if batch_idx % train_config.log_interval == 0 and batch_idx > 0:              \n",
        "            print(\n",
        "                'Train Epoch: {} [{}/{}] Loss: {:.6f} Acc: {:.4f}'.format(\n",
        "                    epoch_idx, batch_idx * len(data), len(train_loader.dataset), loss.item(), acc\n",
        "                )\n",
        "            )\n",
        "            \n",
        "    epoch_loss = batch_loss.mean()\n",
        "    epoch_acc = batch_acc.mean()\n",
        "    return epoch_loss, epoch_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9unZhnL15v_"
      },
      "source": [
        "def validate(\n",
        "    train_config: TrainingConfiguration,\n",
        "    model: nn.Module,\n",
        "    test_loader: torch.utils.data.DataLoader,\n",
        ") -> float:\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    count_corect_predictions = 0\n",
        "    for data, target in test_loader:\n",
        "        indx_target = target.clone()\n",
        "        data = data.to(train_config.device)\n",
        "        \n",
        "        target = target.to(train_config.device)\n",
        "        \n",
        "        output = model(data)\n",
        "        # add loss for each mini batch\n",
        "        test_loss += F.cross_entropy(output, target).item()\n",
        "        \n",
        "        # Score to probability using softmax\n",
        "        prob = F.softmax(output, dim=1)\n",
        "        \n",
        "        # get the index of the max probability\n",
        "        pred = prob.data.max(dim=1)[1] \n",
        "        \n",
        "        # add correct prediction count\n",
        "        count_corect_predictions += pred.cpu().eq(indx_target).sum()\n",
        "\n",
        "    # average over number of mini-batches\n",
        "    test_loss = test_loss / len(test_loader)  \n",
        "    \n",
        "    # average over number of dataset\n",
        "    accuracy = 100. * count_corect_predictions / len(test_loader.dataset)\n",
        "    \n",
        "    print(\n",
        "        '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            test_loss, count_corect_predictions, len(test_loader.dataset), accuracy\n",
        "        )\n",
        "    )\n",
        "    return test_loss, accuracy/100.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6r2Zot317WG"
      },
      "source": [
        "def save_model(model, device, model_dir='models', model_file_name='cifar10_cnn_model.pt'):\n",
        "    \n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "\n",
        "    model_path = os.path.join(model_dir, model_file_name)\n",
        "\n",
        "    # make sure you transfer the model to cpu.\n",
        "    if device == 'cuda':\n",
        "        model.to('cpu')\n",
        "\n",
        "    # save the state_dict\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    \n",
        "    if device == 'cuda':\n",
        "        model.to('cuda')\n",
        "    \n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTWoz3Zv2BAx"
      },
      "source": [
        "def main(system_configuration=SystemConfiguration(), training_configuration=TrainingConfiguration()):\n",
        "    \n",
        "    # system configuration\n",
        "    setup_system(system_configuration)\n",
        "\n",
        "    # batch size\n",
        "    batch_size_to_set = training_configuration.batch_size\n",
        "    # num_workers\n",
        "    num_workers_to_set = training_configuration.num_workers\n",
        "    # epochs\n",
        "    epoch_num_to_set = training_configuration.epochs_count\n",
        "\n",
        "    # if GPU is available use training config, \n",
        "    # else lowers batch_size, num_workers and epochs count\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "    else:\n",
        "        device = \"cpu\"\n",
        "        num_workers_to_set = 2\n",
        "\n",
        "    # data loader\n",
        "    train_loader, test_loader = get_data(\n",
        "        batch_size=training_configuration.batch_size,\n",
        "        data_root=training_configuration.data_root,\n",
        "        num_workers=num_workers_to_set\n",
        "    )\n",
        "    \n",
        "    # Update training configuration\n",
        "    training_configuration = TrainingConfiguration(\n",
        "        device=device,\n",
        "        num_workers=num_workers_to_set\n",
        "    )\n",
        "\n",
        "    # initiate model\n",
        "    model = MyModel()\n",
        "\n",
        "    print(\"Training on {}\".format(training_configuration.device))\n",
        "        \n",
        "    # send model to device (GPU/CPU)\n",
        "    model.to(training_configuration.device)\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=training_configuration.learning_rate\n",
        "    )\n",
        "\n",
        "    # learning scheduler \n",
        "    step_size = 4 # epochs \n",
        "    decay_rate = 0.5\n",
        "    scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=decay_rate)\n",
        "\n",
        "    best_loss = torch.tensor(np.inf)\n",
        "    best_accuracy = torch.tensor(0)\n",
        "    \n",
        "    # epoch train/test loss\n",
        "    epoch_train_loss = np.array([])\n",
        "    epoch_test_loss = np.array([])\n",
        "    \n",
        "    # epch train/test accuracy\n",
        "    epoch_train_acc = np.array([])\n",
        "    epoch_test_acc = np.array([])\n",
        "    \n",
        "    # trainig time measurement\n",
        "    t_begin = time.time()\n",
        "    for epoch in range(training_configuration.epochs_count):\n",
        "        \n",
        "        train_loss, train_acc = train(training_configuration, model, optimizer, train_loader, epoch)\n",
        "        \n",
        "        epoch_train_loss = np.append(epoch_train_loss, [train_loss])\n",
        "        \n",
        "        epoch_train_acc = np.append(epoch_train_acc, [train_acc])\n",
        "\n",
        "        elapsed_time = time.time() - t_begin\n",
        "        speed_epoch = elapsed_time / (epoch + 1)\n",
        "        speed_batch = speed_epoch / len(train_loader)\n",
        "        eta = speed_epoch * training_configuration.epochs_count - elapsed_time\n",
        "        \n",
        "        print(\n",
        "            \"Elapsed {:.2f}s, {:.2f} s/epoch, {:.2f} s/batch, ets {:.2f}s\".format(\n",
        "                elapsed_time, speed_epoch, speed_batch, eta\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # step the scheduler\n",
        "        scheduler.step()\n",
        "        print(\"last LR = {}\".format(scheduler._last_lr))\n",
        "\n",
        "        if epoch % training_configuration.test_interval == 0:\n",
        "            current_loss, current_accuracy = validate(training_configuration, model, test_loader)\n",
        "            \n",
        "            epoch_test_loss = np.append(epoch_test_loss, [current_loss])\n",
        "        \n",
        "            epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\n",
        "            \n",
        "            if current_loss < best_loss:\n",
        "                best_loss = current_loss\n",
        "            \n",
        "            if current_accuracy > best_accuracy:\n",
        "                best_accuracy = current_accuracy\n",
        "                print('Accuracy improved, saving the model.\\n')\n",
        "                save_model(model, device)\n",
        "            \n",
        "                \n",
        "    print(\"Total time: {:.2f}, Best Loss: {:.3f}, Best Accuracy: {:.3f}\".format(time.time() - t_begin, best_loss, \n",
        "                                                                                best_accuracy))\n",
        "    \n",
        "    return model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TikDeVTP2EG4"
      },
      "source": [
        "if required_training:\n",
        "    model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc = main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBWVxzSm2FbL"
      },
      "source": [
        "# Plot loss\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
        "x = range(len(epoch_train_loss))\n",
        "\n",
        "\n",
        "plt.figure\n",
        "plt.plot(x, epoch_train_loss, color='r', label=\"train loss\")\n",
        "plt.plot(x, epoch_test_loss, color='b', label=\"validation loss\")\n",
        "plt.xlabel('epoch no.')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TmFyBwg2HdA"
      },
      "source": [
        "# Plot loss\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
        "x = range(len(epoch_train_loss))\n",
        "\n",
        "\n",
        "plt.figure\n",
        "plt.plot(x, epoch_train_acc, color='r', label=\"train accuracy\")\n",
        "plt.plot(x, epoch_test_acc, color='b', label=\"validation accuracy\")\n",
        "plt.xlabel('epoch no.')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(loc='center right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0O8IHzW2Lve"
      },
      "source": [
        "# initialize the model\n",
        "cnn_model = MyModel()\n",
        "\n",
        "models = 'models'\n",
        "\n",
        "model_file_name = 'cifar10_cnn_model.pt'\n",
        "\n",
        "model_path = os.path.join(models, model_file_name)\n",
        "\n",
        "# loading the model and getting model parameters by using load_state_dict\n",
        "cnn_model.load_state_dict(torch.load(model_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgau6x_g2Nmr"
      },
      "source": [
        "def prediction(model, train_config, batch_input):\n",
        "    \n",
        "    # send model to cpu/cuda according to your system configuration\n",
        "    model.to(train_config.device)\n",
        "    \n",
        "    # it is important to do model.eval() before prediction\n",
        "    model.eval()\n",
        "\n",
        "    data = batch_input.to(train_config.device)\n",
        "\n",
        "    output = model(data)\n",
        "\n",
        "    # Score to probability using softmax\n",
        "    prob = F.softmax(output, dim=1)\n",
        "\n",
        "    # get the max probability\n",
        "    pred_prob = prob.data.max(dim=1)[0]\n",
        "    \n",
        "    # get the index of the max probability\n",
        "    pred_index = prob.data.max(dim=1)[1]\n",
        "    \n",
        "    return pred_index.cpu().numpy(), pred_prob.cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULaNPeD52TGV"
      },
      "source": [
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "batch_size = 5\n",
        "train_config = TrainingConfiguration()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    train_config.device = \"cuda\"\n",
        "else:\n",
        "    train_config.device = \"cpu\"\n",
        "    \n",
        "    \n",
        "\n",
        "# load test data without image transformation\n",
        "test = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10(root=train_config.data_root, train=False, download=False, \n",
        "                   transform=transforms.functional.to_tensor),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=1\n",
        "    )\n",
        "\n",
        "try:\n",
        "    mean, std = get_mean_std_train_data(data_root)\n",
        "    assert len(mean) == len(std) == 3\n",
        "except:\n",
        "    mean = (0.5, 0.5, 0.5)\n",
        "    std = (0.5, 0.5, 0.5)\n",
        "\n",
        "# load testdata with image transformation\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "    ])\n",
        "\n",
        "test_trans = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10(root=train_config.data_root, train=False, download=False, transform=image_transforms),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=1\n",
        "    )\n",
        "\n",
        "for data, _ in test_trans:\n",
        "    # pass the loaded model\n",
        "    pred, prob = prediction(cnn_model, train_config, data)\n",
        "    break\n",
        "    \n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (3, 3)\n",
        "for images, label in test:\n",
        "    for i, img in enumerate(images):\n",
        "        img = transforms.functional.to_pil_image(img)\n",
        "        plt.imshow(img)\n",
        "        plt.gca().set_title('Pred: {0}({1:0.2}), Label: {2}'.format(classes[pred[i]], prob[i], classes[label[i]]))\n",
        "        plt.show()\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_Q530Aw5y_x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}