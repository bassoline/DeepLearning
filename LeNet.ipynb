{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LeNet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP3LXZw/ZvxW+rs4gLhWjIV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bassoline/DeepLearning/blob/main/LeNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1GHTNpM-PEv"
      },
      "source": [
        "%matplotlib inline \n",
        "import matplotlib.pyplot as plt \n",
        "import os \n",
        "import time \n",
        "from typing import Iterable \n",
        "from dataclasses import dataclass \n",
        "import numpy as np \n",
        "import torch \n",
        "import torch.nn as nn \n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dI_AR6uXxPJq"
      },
      "source": [
        "class LeNet5(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # convolution layers\n",
        "    self._body = nn.Sequential(\n",
        "        # input size = (32, 32), output size = (28, 28)\n",
        "        nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=2), \n",
        "        # second conv layer\n",
        "        # input size (14, 14), output size = (10, 10)\n",
        "        nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=2) \n",
        "        # output is (5, 5)\n",
        "    )\n",
        "\n",
        "    # fully connected layers \n",
        "    self._head = nn.Sequential(\n",
        "        # in features = total # of weights in last conv layer\n",
        "        nn.Linear(in_features=16*5*5, out_features=120), \n",
        "        nn.ReLU(inplace=True),\n",
        "        # second fully connected layer\n",
        "        nn.Linear(in_features=120, out_features=84),\n",
        "        nn.ReLU(inplace=True),\n",
        "        # last fully connected layer\n",
        "        nn.Linear(in_features=84, out_features=10)\n",
        "    )\n",
        "\n",
        "  def foward(self, x): \n",
        "    # apply feature extractor (conv layers)\n",
        "    x = self._body()\n",
        "    # flatten output of conv layer (dim should be batch_size * # of weights in \n",
        "    # last conv layer)\n",
        "    x = x.view(x.size()[0], -1) \n",
        "    # apply classifier \n",
        "    x = self._head(x)\n",
        "    return x \n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBP87m7n0Kx-",
        "outputId": "4873f5ef-fa9d-473c-ff5f-7efd907155a3"
      },
      "source": [
        "# initalize and display network \n",
        "lenet5_model = LeNet5()\n",
        "print(lenet5_model)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LeNet5(\n",
            "  (_body): Sequential(\n",
            "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (_head): Sequential(\n",
            "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sw0mjRhJ0ZaN"
      },
      "source": [
        "# get mnist data \n",
        "def get_data(batch_size, data_root='data', num_workers=1):\n",
        "  # transform the data first, resize and normalize\n",
        "  data_transform = transforms.Compose([\n",
        "                                             transforms.Resize((32, 32)),\n",
        "                                             # rescales to value between 0-1\n",
        "                                             transforms.ToTensor(), \n",
        "                                             # first number is the mean and the second # is the variance (pre calculated)\n",
        "                                             transforms.Normalize((0.1307, ), (0.3081, ))                                        \n",
        "  ])\n",
        "\n",
        "  # create the loaders\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "      datasets.MNIST(root=data_root, train=True, download=True, transform=data_transform), \n",
        "      batch_size = batch_size, \n",
        "      shuffle=True, \n",
        "      num_workers = num_workers\n",
        "  )\n",
        "\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "      datasets.MNIST(root=data_root, train=False, download=True, transform=data_transform),\n",
        "      batch_size=batch_size, \n",
        "      shuffle=False, \n",
        "      num_workers=num_workers\n",
        "  )\n",
        "\n",
        "  return train_loader, test_loader"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-A4lJag37H_"
      },
      "source": [
        "# common settings for reproducibility \n",
        "@dataclass\n",
        "class SystemConfiguration:\n",
        "  seed: int = 42 # for rand nums \n",
        "  cudnn_benchmark_enabled: bool = True # for performance\n",
        "  cudnn_deterministic: bool = True # reproducibility"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp1HalmB4bkd"
      },
      "source": [
        "@dataclass\n",
        "class TrainingConfiguration:\n",
        "  batch_size: int = 32 # amount of data passed through each iteration \n",
        "  epochs_count: int = 20 # of times all data will be passed through\n",
        "  learning_rate: float = 0.01 # how fast we update the weights based on the gradients\n",
        "  log_interval: int = 100 # how many batches between logging status\n",
        "  test_interval: int = 1 # how many epoches to wait before another evaluation test \n",
        "  data_root: str = \"data\" # folder to save MNIST data at (data/mnist-data)\n",
        "  num_workers: int = 10 # of concurrent processes used to prepare data\n",
        "  device: str = 'cuda' # device used for training "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7I8-WwJ5vb4"
      },
      "source": [
        "# checks for GPU availability and sets up the system for you \n",
        "def setup_system(system_config: SystemConfiguration) -> None: \n",
        "  torch.manual_seed(system_config.seed)\n",
        "  if torch.cuda.is_available():\n",
        "    torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n",
        "    torch.backends.cudnn.deterministic = system_config.cudnn_deterministic"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3Kb4on76qaV"
      },
      "source": [
        "# training \n",
        "def train(\n",
        "    train_config: TrainingConfiguration, model: nn.Module, optimizer: torch.optim.Optimizer,\n",
        "    train_loader: torch.utils.data.DataLoader, epoch_idx: int\n",
        ") -> None: \n",
        "  # set model to train mode\n",
        "  model.train() \n",
        "  # for storing loss and accuracy \n",
        "  batch_loss = np.array([])\n",
        "  batch_acc = np.array([])\n",
        "\n",
        "  for batch_idx, (data, target) in enumerate(train_loader): \n",
        "    # clone the target \n",
        "    indx_target = target.clone()\n",
        "    # send data and target to device (for GPU) doesn't hurt if there is not GPU\n",
        "    data = data.to(train_config.device)\n",
        "    target = train.to(train_config.device)\n",
        "    # set gradient to zero \n",
        "    optimizier.zero_grad()\n",
        "    # forward pass \n",
        "    output = model(data)\n",
        "    # cross entropy loss \n",
        "    loss = F.cross_entropy(output, target)\n",
        "    "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mpxmP-Y6ubF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}